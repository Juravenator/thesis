\chapter{TS 3.0 upgrade requirements}
The previous chapter discussed the problems with TS version 2.0.

It is desirable to mitigate all these problems and prepare TS version 3.0 for
the future.
Several requirements were submitted to the then hypothetical new TS 3.0
components.

\section{Legacy code compatibility}
Currently there are many panels developed in the legacy TS. It is unfeasible to
upgrade or rewrite these all in one go. There will be a transitional period where
legacy code will have to run concurrently with newer code.

Therefore TS 3.0 must maintain as much code compatibility with TS 2.0 as possible.
It is very desirable to have 100\% compatibility. A slight area of code
incompatibility might have large consequences depending on legacy panel code.

This will put some restraints on the upgrade options as full legacy code
compatibility requires the new codebase to be a superset of the old one.
However this constraint is mostly applicable only to the server side code, as
the client side code is generated by the server and can be significantly
modified provided developers keep watchful of any changes.

\section{Ability to migrate code}
As the legacy code will be converted to new code, it would be desirable to
make the transition as easy as possible and not to have a too much difference
between modern and legacy code as far as keeping existing functionality is
concerned.

New functionality will of course result in new code. This requirement therefore
only applies to migrating legacy code to keep the functionality as it was.

\section{Future proof}
Web technologies are moving forwards in a fast pace. A lot of new standards have
arisen for us to use and it would be wise to use them.

Designing a codebase that uses as much open standards as possible is a good
practice. It ensures good support from communities using those standards, and
ensures the codebase will maintain compatibility with web browsers for a far
longer period than would otherwise be possible.

Provided finalized open standards are used, it would be acceptable to use
relatively modern technologies and currently heavily rely on polyfills, libraries
designed to emulate a spec not yet implemented, to provide the needed
compatibility with currently used software.

\subsection{Polyfills}
A polyfill is a term used in web development. It is a group of JavaScript
libraries designed with the very specific use case to implement a future
standard, or even just a working draft of a standard, as accurately as possible
with today's resources. Some of these also seek to fix a broken implementation
of a standard by specific browsers.

Polyfills have gained a lot of popularity the last few years, approximately in
tandem with the upcoming of `evergreen` browsers, as web development now mainly
focuses on building on open standards rather than focus on a specific browser
or even a specific version of a browser.

Polyfills are generally allowed to introduce as much CPU, memory, and network
usage as needed to implement their targeted standard as completely as possible.
The main argument for this is that a polyfill is designed to be obsolete after
web browsers have caught up and implemented said spec. At this point a polyfill
is designed to no longer be activated anymore, mitigating the originally
introduced loads.

\section{Rich functionality}
One of the main reasons to renew the codebase of the TS is to be able to fulfill
the new modern requirements for the interface.

A lot of new features are needed, like the ability to handle large datasets and
the ability to handle more complex analysis use cases.

It would be desirable to have an extendable framework. This way, as time goes on
and requirements change, the framework can be adapted and extended to handle
new requirements as needed.

\section{Faster development}
The new framework must be much faster to develop on, as currently it takes weeks
to implement any change whatsoever.

The current main inhibitors of development time are the lack of features and
the amount of unreadable code that the use of the framework causes.

Given the main problems of the slow development time it can be expected that,
whatever the new framework looks like, will introduce a major improvement of
development time of new interface panels.

\section{Stability}
The new codebase will have sessions at the CMS Control Centre that last for days.
This puts important requirements on front-end libraries and frameworks.

Memory leaks are unacceptable. A memory leak results in an unstable interface,
which is unacceptable during operations in the Control Centre.
This is explained in more detail in chapter \ref{Memory-leak problem}.

\section{Reduced code footprint}
The current code of an interface panel is too large and too messy.
It makes the code unreadable and very difficult to maintain.

It would be desirable to have a far smaller code footprint for basic panel
functionality. It is a sign of a more powerful framework and will ease the
later modifications to panel code.

A smaller amount of required code would translate to the need for a more powerful
framework. It must however not introduce functionality that abstracts
away it's functionality so much that it would introduce `black magic` code, i.e.
code that works but nobody knows why\cite{jargonfile}.

\section{Better maintainability}
Functional requirements change regularly. Whatever the new framework looks like
must be flexible and open enough to be extended or modified to provide for new
functionality as needed. This must either be done in-house or by an extensive and
stable developer community associated with the framework.

A `dead` framework, i.e. one that has no more developer community or ability to
be easily modified in-house, like happened with Dojo 0.4 must be avoided.

\section{Better documentation}
The previous requirement of maintainability puts a strong emphasis on documentation.
A system can't be properly maintained, nor modified, if the system's workings
aren't properly explained.

The framework will need extensive documentation describing the possible ways to
program panels and showcase advanced functionalities.
Documentation must also exist about the inner workings of the framework.

Documentation must also be easily kept synchronized with the actual state of the
code. This would suggest the use of inline documentation, i.e. documentation that
resides in the source code.

\section{i18n}
Given the multilingual environment this framework will be used in, it is
useful to have a codebase that can adapt to different interface languages.

\section{Browser compatibility}
The TS 3.x front-end libraries must be able to operate on CERN supported browsers.
This means it must support any browser installed by default on the currently
supported CERN Virtual Machines (VMs). Currently this is Scientific Linux CERN 6
(SLC6) (\url{http://linux.web.cern.ch/linux/scientific5/}).

This, along with user requests, gives us the following list of browsers that
need supporting and their minimum required versions.
\begin{itemize}[noitemsep]
\item Mozilla Firefox Extended Support Release (ESR) 24-45
\item Apple Safari 9
\item Google Chrome (latest version)
\end{itemize}

Notable is the absence of Microsoft Internet Explorer (MS IE) from this list.
This is caused by the fact that all production systems use SLC and thus do not
run Microsoft Windows.

The Opera and Vivaldi browser shares the same JavaScript and rendering engine as Google Chrome
since 2013, when Opera changed to the Blink engine. This also applies to the
Vivaldi browser, a popular alternative to the Opera browser.
This means that if Google Chrome support is achieved, by definition also
Opera/Vivaldi support is achieved.
